---
title: "Practical Machine Learning Course Project"
author: "MARTIN Thomas"
output: html_document
---

## Background 

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Overview

The purpose of this document is to determine some models to predict from accelerometers data, the way participants perform the barbell lift. Then For each establish the accuracy (in sample/out of the sample) and the training time. Finally, choose the one to be able to predict the classes of observations from other variables of observations.
The found model will be applied to the 20 test observations.

Before this, a cleaning of data will be applied in order to have 
efficient analysis.

## Library loading & constants

```{r library, echo=TRUE, message=FALSE, warning=FALSE}
library(dplyr)
library(caret)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(party)
library(rattle)

URL_TRAINING_SET = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
URL_TESTING_SET = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
```

## Retrieving & cleaning data

We retrieve the two set data from the web using read.csv function with "" & NA as NA value.

```{r get_data, echo=TRUE, message=FALSE, cache =TRUE}
trainRawData <- read.csv(URL_TRAINING_SET, na.strings=c("", "NA"))
testRawData <- read.csv(URL_TESTING_SET, na.strings=c("", "NA"))
```

First of all, we verify if the trainData and testData has the same variables (except the last column which is specific to each data set | classe for training set : the outcome to predict,  problem_id for testing set)

```{r verification, echo=TRUE}
trainNames <- colnames(trainRawData)[-dim(trainRawData)[2]] #Remove of last column
testNames <- colnames(testRawData)[-dim(testRawData)[2]] #Remove of last column
all.equal(trainNames, testNames) # Comparison of columns name for both sets.
```
The both set has the same variables.

Then we count the NA value for each column and display the table of the result.
```{r cleaning_data_1, echo=TRUE}
na_percent <-sapply(trainRawData, function(y) (100 * sum(length(which(is.na(y)))))/
                                dim(trainRawData)[1])
na_percent <- round(na_percent, 2)
table(na_percent)
``` 

So, there is 100 variables with 97.93 % of NA. Those variables will not be helpful
to build predicting models. We can remove them. 
We will apply the same cleaning to test observations.

```{r cleaning_data_2, echo=TRUE}
trainData <- trainRawData[, na_percent == 0]
testData <- testRawData[, na_percent == 0]
```

Then the 7 first columns (`r colnames(trainData)[1:7]`) are also not useful 
to build a predicting model, they will be removed.

```{r cleaning_data_3, echo=TRUE}
trainData <- trainData[,8:dim(trainData)[2]]
testData <- testData[,8:dim(testData)[2]]
```

## Building models

Now, we split the trainData to have a training set and a model cross-validation set.

```{r building, echo=TRUE}
set.seed(4242)
inTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)
trainSet <- trainData[inTrain,]
validationSet <- trainData[-inTrain,]
```

The training set has `r dim(trainSet)[1]` observations. The cross-validation set has
`r dim(validationSet)[1]` observations.

Now, we have our data ready, we can try different predicting model for classification.

### CART modeling via rpart
We begin by using CART (Classification and regression tree)

We first consider the model without any pre-processing options. 
```{r CART, echo=TRUE, cache=TRUE}
set.seed(4242)
timeMod1 <- system.time(mod1 <- train(classe~ ., data = trainSet,  method="rpart"))
predMod1 <- predict(mod1, validationSet[, -c(dim(validationSet)[2])])
confMod1 <- confusionMatrix(predMod1, validationSet$classe)
```

```{r CART_plot, echo=TRUE}
fancyRpartPlot(mod1$finalModel)
confMod1
``` 

From the confusion matrix and the low out of sample accuracy, we can already have the intuition that this model is not a good one.

### Random Forest modeling via rf

```{r rf, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE}
set.seed(4242)
timeMod2 <- system.time(mod2 <- train(classe~ ., data = trainSet,  method="rf", 
                                      trControl=trainControl(method = "cv", number = 4)))
predMod2 <- predict(mod2, validationSet[, -c(dim(validationSet)[2])])
confMod2 <- confusionMatrix(predMod2, validationSet$classe)
```

```{r rf_plot, echo=TRUE}
confMod2
``` 

From the confusion matrix, we can observe that model has a excellent accuracy.

### Gradient Boosting modeling via gbm

```{r gbm, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE}
set.seed(4242)
timeMod3 <- system.time(mod3 <- train(classe~ ., data = trainSet,  method="gbm", verbose = FALSE))
predMod3 <- predict(mod3, validationSet[, -c(dim(validationSet)[2])])
confMod3 <- confusionMatrix(predMod3, validationSet$classe)
```

```{r gbm_plot, echo=TRUE}
confMod3
``` 

From the confusion matrix, we can observe that model has a correct accuracy.

### Comparisons
```{r comparison, echo=TRUE}
model_name <- c("CART", "Random Forest", "Gradient Boost")
training_time <- c(timeMod1[3], timeMod2[3], timeMod3[3])
in_sample_accuracies <- c(round(mod1$results[1,2], 3), 
                          round(mod2$results[1,2], 3),
                          round(mod3$results[1,2], 3))
out_sample_accuracies <- c(round(confMod1$overall[1], 3), 
                              round(confMod2$overall[1], 3), 
                              round(confMod3$overall[1], 3))
recap <- data.frame(model_name, training_time, in_sample_accuracies, out_sample_accuracies)
recap
```     

CART model is the fastest one to train but its accuracy is really low for both in/out samples. It can be discarded without any mercy.  
The Gradient Boost model perfectly fit the training sample, but that is not what we really want. The out of sample accuracy is correct but not the best and this model has a huge training time (compared to others), the worst of the 3 tested models.  
The Random Forest model fits excellently the training sample, fits even more excellently the test set, and have a correct training time (compared to others).

Random Forest model will be the selected one to apply to the 20-observations test sample.

## Conclusion

So, using the most accurate model for out of sample data (i.e. Random Forest model.), we apply it to the test sample and get the following result :

```{r conclusion, echo=TRUE, message=FALSE, warning=FALSE}
predTest <- predict(mod2, testData[, -dim(testData)[2]])
predTest
```
